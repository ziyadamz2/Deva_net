# Multimodal Sentiment Analysis â€“ MOSI (Texte + Audio + VidÃ©o)






Ce projet implÃ©mente un modÃ¨le de **sentiment multimodal** capable dâ€™analyser simultanÃ©ment :

- ğŸ“ **Texte**  
- ğŸ¤ **Audio**  
- ğŸ¥ **VidÃ©o (frames)**  
- ğŸ˜¶ **Features OpenFace (AUs / Vision)**  
- ğŸ—£ï¸ **Descriptions textuelles des modalitÃ©s (D_a, D_v,D_ann)**  
- ğŸ“Œ **Annotations humaines**

Le modÃ¨le fusionne ces modalitÃ©s via un rÃ©seau **Fully Connected Multimodal (DevaModel)** et prÃ©dit un score de sentiment continu.

---

## ğŸš€ FonctionnalitÃ©s ImplÃ©mentÃ©es

### ğŸ”¹ 1. **Encodage du Texte**
- BERT (HuggingFace) pour encoder :  
  - transcription (`X_t`)  
  - description audio (`D_a`)  
  - description vidÃ©o (`D_v`)  
  - annotation humaine (`D_ann`)

### ğŸ”¹ 2. **Encodage Audio**
- Chargement des signaux audio (.npy)  
- Extraction MFCC / embeddings (selon preprocessing)

### ğŸ”¹ 3. **Encodage VidÃ©o**
- Chargement des frames vidÃ©o par segment  
- Chargement des AUs (vision features) depuis `mosi_data.pkl`

### ğŸ”¹ 4. **Fusion Multimodale**
ModÃ¨le personnalisÃ© :
- Projection de toutes les modalitÃ©s  
- ConcatÃ©nation  
- MLP final â†’ prÃ©diction du score de sentiment

### ğŸ”¹ 5. **Deux versions de modÃ¨les EnregistrÃ©s**
- `deva_model_30_non_normaliser_Emotional_Embedding.pth`  (version finale)
- `deva_model_30_epoch_normaliser_Bert_Embedding.pth` 

---

## ğŸ“¦ DonnÃ©es UtilisÃ©es

Les donnÃ©es viennent du dataset **CMU-MOSI**, un benchmark multimodal pour le sentiment.

### ğŸ“ Sources utilisÃ©es dans ce projet :
- `CMU-MOSI/label.csv` â†’ textes, annotations, labels  
- `mosi_data.pkl` â†’ vision features (AUs) + IDs  
- `train_mosi_frames_list.npy` â†’ frames vidÃ©o (train)  
- `train_mosi_audio_list.npy` â†’ audios (train)  
- `test_mosi_frames_list.npy` â†’ frames vidÃ©o (test)  
- `test_mosi_audio_list.npy` â†’ audios (test)

---

## ğŸ”— Lien vers la Data

Toutes les donnÃ©es utilisÃ©es dans ce projet sont accessibles ici :

ğŸ‘‰ **Google Drive (DonnÃ©es + ModÃ¨les)**  
https://drive.google.com/drive/folders/1StfuzhPdk-6V1JLLF7u0C0HhQr25ZqwH

---

## ğŸ§  Organisation du Dataset

| ModalitÃ© | Format | Description |
|----------|--------|-------------|
| Texte | `label.csv` | Transcriptions + annotations + score (-3 â†’ +3) |
| Audio | `.npy` | Liste des signaux ou MFCC |
| VidÃ©o (frames) | `.npy` | Images extraites de chaque segment |
| Vision / AUs | `mosi_data.pkl` | OpenFace features |
| IDs MOSI | `mosi_data.pkl` | Identifiants de segments |

---
# ğŸ”¥ Fusion Multimodale ImplÃ©mentÃ©e

Le cÅ“ur du projet repose sur une **fusion multimodale hiÃ©rarchique**, composÃ©e de :

### ğŸ”¹ 1. **Projection indÃ©pendante**
Chaque modalitÃ© est dâ€™abord projetÃ©e dans un espace commun :

- BERT â†’ vectorisation du texte, D_a, D_v, D_ann  
- Audio MFCC â†’ vecteur 20D  
- Vision â†’ vecteur OpenFace  
- Frames vidÃ©o â†’ embeddings prÃ©-extraits  

Chaque vecteur passe dans une couche linÃ©aire :  


---

## ğŸ§ª EntraÃ®nement & Tests

- EntraÃ®nement effectuÃ© pendant **30 Ã©poques**
- Version finale :  
  `deva_model_30_epoch_normaliser_Bert_Embedding.pth`
- PrÃ©diction dâ€™un score continu de sentiment

---

## ğŸ“œ Licence
Projet acadÃ©mique â€” ECE Paris.

---

## ğŸ‘¤ Auteur
Ziyad â€” Projet PFE Multimodal Sentiment Analysis


